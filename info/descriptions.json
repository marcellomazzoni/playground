{ 
  "general": {
    "test_size": "This parameter determines the proportion of the dataset to hold back as the final **test set**. The model is trained on the remaining data (the training set) and then evaluated on this unseen test set to get an unbiased estimate of its performance.\n\n- **Common values**: A split of 20-30% for the test set is typical, leaving 70-80% of the data for training.",
    "cv_folds": "**K-Fold Cross-Validation** is a robust technique for tuning and evaluating models. The training data is split into 'K' equal parts or 'folds'. The model is then trained K times, each time using one fold for validation and the remaining K-1 folds for training. The final performance is the average of the results from the K folds.\n\n- **The value of K**: A higher K gives a more reliable performance estimate but is computationally more expensive. `K=5` or `K=10` are common choices.",
    "random_state": "This parameter sets a seed for the random number generator, ensuring that any operation with a random component produces the **exact same results** every time you run the script. This is crucial for reproducibility.\n\n- **Why use it?**: It guarantees that data splits, model initializations, and random sampling are identical across runs. Any integer can be used as a seed, with `42` being a popular convention."
  },

  "knn": {
    "n_neighbors": "This parameter, **k**, defines the number of nearest neighbors the algorithm will consider when making a prediction for a new data point. It's a critical parameter that controls the bias-variance tradeoff.\n\n- **Small k (e.g., 3, 5)**: The model is more flexible and sensitive to local patterns, but it can also be noisy and prone to overfitting.\n- **Large k (e.g., 9, 11)**: The model is more stable and less affected by outliers, but it might smooth over important local details, potentially underfitting the data.",
    "weights": "Determines how the votes of the neighbors are weighted when making a prediction. You can choose how much influence each neighbor has.\n\n- **uniform**: All neighbors are treated equally. Each of the *k* neighbors gets one vote, regardless of how close or far they are.\n- **distance**: Closer neighbors have a stronger influence. The vote of each neighbor is weighted by the inverse of its distance, so nearer points contribute more to the final prediction.",
    "metric": "This defines the formula used to calculate the distance between data points, which is the core of how KNN finds 'neighbors'. The choice of metric should match the nature of your data.\n\n- **euclidean**: The most common choice. It's the straight-line distance between two points ('as the crow flies'). A great default for many problems.\n- **manhattan**: Also known as 'city block' distance. It measures the distance by summing the absolute differences of the coordinates. Useful for grid-like data.\n- **cosine**: Calculates the angle between two data points. It's very effective for text analysis or when the magnitude of the data doesn't matter, only its orientation.\n- **chebyshev**: The maximum distance along any single dimension. Useful in specific cases where the largest difference in one feature is the most important factor."
  },

  "linear_models": {
    "regularization": "Regularization is a technique used to prevent overfitting by adding a penalty to the model's loss function. This discourages overly complex models where the coefficients are too large.\n\n- **None**: A standard linear model with no penalty. It can perform poorly if features are correlated or irrelevant.\n- **Ridge (L2)**: Adds a penalty proportional to the square of the coefficients' magnitude. This shrinks coefficients but rarely makes them exactly zero. It's effective when many features are useful.\n- **Lasso (L1)**: Adds a penalty proportional to the absolute value of the coefficients. This can force some feature coefficients to become exactly zero, effectively performing automatic feature selection.",
    "alpha": "**Alpha (α)** is the hyperparameter that controls the strength of the regularization penalty. It balances between fitting the data well and keeping the model's coefficients small.\n\n- **Small Alpha (e.g., 0.1)**: The penalty is weak. The model behaves more like an unregularized linear model, with a higher risk of overfitting.\n- **Large Alpha (e.g., 100.0)**: The penalty is strong, forcing the coefficients to shrink significantly towards zero. This creates a simpler model but carries a risk of underfitting."
  },

  "random_forest": {
    "n_estimators": "This is the number of decision trees in the forest. A Random Forest combines multiple trees to produce a more accurate and stable prediction.\n\n- **More trees (e.g., 200, 300)**: Generally leads to better performance and reduces overfitting, but increases the training time and computational cost.\n- **Fewer trees (e.g., 10, 20)**: Faster to train but may not be as robust or accurate.",
    "max_depth": "This parameter controls the maximum depth of each individual tree. It's one of the primary ways to control the model's complexity and prevent overfitting.\n\n- **Shallow depth (e.g., 5, 10)**: Creates simpler trees that generalize well but might underfit if the data structure is complex.\n- **Deep depth (e.g., 20, None)**: Allows trees to grow very large and capture fine-grained details in the data, but carries a high risk of overfitting. `None` means nodes are expanded until all leaves are pure.",
    "min_samples_split": "Specifies the minimum number of samples an internal node must have before it can be split into further nodes. This parameter helps control the granularity of the tree splits.\n\n- **Small value (e.g., 2)**: Allows the tree to continue splitting even on small groups of data, potentially leading to overfitting.\n- **Large value (e.g., 10)**: Prevents the model from learning relationships that are only supported by a very small number of samples, making the model more general and robust."
  },

  
  "logistic_regression": {
    "C": "The **C** parameter controls the inverse of the regularization strength. It's the opposite of the 'alpha' parameter in models like Ridge or Lasso.\n\n- **Small C (e.g., 0.1)**: Corresponds to **stronger** regularization. This creates a simpler model that is less likely to overfit.\n- **Large C (e.g., 100)**: Corresponds to **weaker** regularization. The model will fit the training data more closely, which can increase the risk of overfitting.",
    "penalty": "Specifies the type of regularization penalty to apply to the model's coefficients. Your choice of penalty must be compatible with the selected solver.\n\n- **l2**: The standard Ridge penalty. It shrinks coefficients but doesn't eliminate them. Supported by most solvers.\n- **l1**: The Lasso penalty. It can force some coefficients to become exactly zero, performing feature selection.\n- **elasticnet**: A combination of L1 and L2 penalties. Only the 'saga' solver supports this.\n- **none**: No regularization is applied.",
    "solver": "The **solver** is the algorithm used to find the optimal model parameters. Different solvers have different strengths and support different penalties. **Note:** If you select incompatible solver-penalty pairs, those combinations will be skipped during training.\n\nHere are the supported penalties for each solver:\n- **lbfgs**: Supports `l2`, `none`\n- **newton-cg**: Supports `l2`, `none`\n- **newton-cholesky**: Supports `l2`, `none`\n- **sag**: Supports `l2`, `none`\n- **liblinear**: Supports `l1`, `l2` (Good choice for smaller datasets)\n- **saga**: Supports `l1`, `l2`, `elasticnet`, `none` (Good choice for very large datasets)"
  },
  
  "naive_bayes": {
    "var_smoothing": "This parameter is a stability adjustment for the Gaussian Naive Bayes model. It adds a small, artificial value to the variance of each feature.\n\nIts primary purpose is to prevent numerical issues that can arise if a feature has zero variance (i.e., all its values are the same for a given class). Without this smoothing, calculations could involve division by zero.\n\n- **Small value (e.g., 1e-9)**: The default. Provides minimal smoothing and is usually sufficient.\n- **Larger value (e.g., 1e-5)**: Increases the smoothing effect, which can sometimes help the model generalize better if the data is noisy."
  },

    "svm": {
        "C": "The **C** parameter is the regularization parameter, which controls the trade-off between achieving a low training error and creating a simple decision boundary. It's a key factor in avoiding overfitting.\n\n- **Small C (e.g., 0.1)**: Creates a 'soft margin', allowing more misclassifications in exchange for a wider, simpler margin. This can improve generalization.\n- **Large C (e.g., 100)**: Creates a 'hard margin', attempting to classify every training point correctly. This can lead to a more complex model that may overfit.",
        "kernel": "The kernel function transforms the input data into a higher-dimensional space, allowing the SVM to find non-linear decision boundaries. This is the core of the 'kernel trick'.\n\n- **linear**: Creates a simple hyperplane. Fast and effective for linearly separable data.\n- **rbf (Radial Basis Function)**: A popular and powerful default that can model complex, non-linear relationships.\n- **poly**: Creates a polynomial decision boundary. Its complexity is controlled by the 'degree' parameter.\n- **sigmoid**: Another non-linear kernel, often used in neural network contexts.",
        "degree": "This parameter specifies the degree of the polynomial for the 'poly' kernel. It controls the flexibility of the decision boundary.\n\n⚠️ **Note:** This option is **only used when 'poly' is selected** as the kernel. A higher degree can fit more complex data but also increases the risk of overfitting.",
        "gamma": "The gamma parameter defines the influence of a single training example. It is a crucial parameter for non-linear kernels.\n\n- **scale**: The recommended default. It automatically adjusts gamma based on the number of features and the data's variance, often leading to better performance.\n- **auto**: A simpler method that only uses the number of features to determine the gamma value.\n\n⚠️ **Note:** This parameter is **only used by the 'rbf', 'poly', and 'sigmoid' kernels**. It has no effect on the 'linear' kernel."
  },

  "xgboost": {
    "n_estimators": "This is the number of boosting rounds, or simply, the number of trees to build. XGBoost builds trees sequentially, with each new tree correcting the errors of the previous ones.\n\n- **More trees**: Generally improves model performance, but also increases the risk of overfitting and the time required for training. This parameter is often tuned in conjunction with the `learning_rate`.",
    "max_depth": "Controls the maximum depth of each individual tree. This is a critical parameter for preventing overfitting.\n\n- **Shallow depth (e.g., 3, 5)**: Creates simpler, more generalized models that are less likely to overfit.\n- **Deep depth (e.g., 7, 9)**: Allows the model to learn more specific and complex patterns, but significantly increases the risk of overfitting.",
    "learning_rate": "Also known as 'eta', this parameter scales the contribution of each new tree. A smaller learning rate shrinks the influence of each tree, requiring more trees to be added to the model.\n\n- **Low value (e.g., 0.01)**: Makes the boosting process more conservative, requiring a higher `n_estimators` but often leading to a more robust model.\n- **High value (e.g., 0.2)**: The model learns faster but is more prone to overfitting.",
    "subsample": "This parameter defines the fraction of the training data (rows) to be randomly sampled for growing each tree. This introduces randomness to help prevent overfitting.\n\n- **Value of 1.0**: All training data is used for each tree.\n- **Value < 1.0 (e.g., 0.8)**: Each tree is trained on a random 80% of the data, which can make the model more robust to noise.",
    "colsample_bytree": "This defines the fraction of features (columns) to be randomly sampled when constructing each tree. It's another effective technique for preventing overfitting.\n\n- **Value of 1.0**: All features are considered for each tree.\n- **Value < 1.0 (e.g., 0.8)**: A random 80% of features are used, encouraging diversity among the trees and preventing the model from relying too heavily on a few dominant features."
  }

}
